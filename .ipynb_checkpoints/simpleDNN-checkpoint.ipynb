{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples.tutorials'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5566aefb4355>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MNIST_data/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples.tutorials'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "# DNN 모델과 최적화 함수에 따라 변경 필요.\n",
    "# cost 값이 작아지면 learning rate 도 작게 변경해보면 좋음.\n",
    "# 0.1 -> 0.01 -> 0.001\n",
    "learning_rate = 0.001\n",
    "# 15 -> 30\n",
    "training_cnt = 30\n",
    "batch_size = 100\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# drop out 에 사용할 값을 담을 변수정의\n",
    "# 학습할 때는 0.5 ~ 0.7 정도의 뉴런을 활성화\n",
    "# 테스트나 상용에서는 1의 뉴런을 활성화 하도록 설정할 예정.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# DNN 모델로 변경\n",
    "# L2 ~ L4 가 Hidden Layer\n",
    "# 중간단계 shape 는 512 (반복 테스트를 통해 튜닝가능)\n",
    "\n",
    "# 정확도를 높이기 위해 Xavier Initializer 사용하여 가중치를 초기화했다가,\n",
    "# 자비에 함수는 양끝이 수렴하는 경우 적절하므로,\n",
    "# ReLU 활성화 함수에 적합한 He 함수를 사용하도록 initializer 부분을 추가해줌.\n",
    "# 이를 위해 get_variable() 함수를 사용.\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.keras.initializers.he_normal())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "# 활성화 함수로 ReLU 사용.\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "# over fitting 이 일어나지 않도록 중간중간 무작위로 뉴런을 비활성화하여 조금더 일반화 시킴.\n",
    "# 트레이닝 데이터셋의 정확도는 다소 떨어지더라도\n",
    "# 새로운 데이터에 대한 정확도는 상승.\n",
    "# tf.layer.dropout() 함수 사용법도 알아보시길..\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.keras.initializers.he_normal())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.keras.initializers.he_normal())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.keras.initializers.he_normal())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.keras.initializers.he_normal())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logits = tf.matmul(L4, W5) + b5\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "# 현재까지 연구된 최적화 함수중 가장 성능이 좋다고 평가됨.\n",
    "# Adam Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "op_train = optimizer.minimize(cost)\n",
    "\n",
    "pred = tf.nn.softmax(logits)\n",
    "prediction = tf.argmax(pred, 1)\n",
    "true_Y = tf.argmax(Y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_cnt):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # 학습 구간에서는 drop out 에 사용될 값을 0.7로 설정함.\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, op_train], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# 테스트 구간에서는 drop out 에 사용될 값을 1로 설정함.\n",
    "print('Accuracy(train):', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.train.images, Y: mnist.train.labels, keep_prob: 1.0}))\n",
    "\n",
    "print('Accuracy(test):', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0}))\n",
    "\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    prediction, feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
